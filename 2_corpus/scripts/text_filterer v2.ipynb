{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cad9f7b",
   "metadata": {
    "id": "8cad9f7b"
   },
   "source": [
    "# Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab752b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install pandarallel\n",
    "# !pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92822196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T02:54:42.093196Z",
     "start_time": "2021-04-18T02:54:41.733640Z"
    },
    "code_folding": [
     55
    ],
    "id": "92822196",
    "outputId": "1cd262fe-85e8-4745-e82a-ae6d152bcadd"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "pd.set_option(\"max_colwidth\", 600)\n",
    "import ast\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "import html\n",
    "import re\n",
    "import glob\n",
    "import pathlib\n",
    "import unicodedata\n",
    "import tarfile\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "from unidecode import unidecode\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "def get_csv_size(csv_name):\n",
    "    get_size = os.path.getsize(os.getcwd() + '/'+csv_name)\n",
    "    mb_size = get_size/(1024 * 1024)\n",
    "    mb_size = round(mb_size,1)\n",
    "    return mb_size\n",
    "\n",
    "def get_attribute(filename):\n",
    "    if filename.endswith('csv'):\n",
    "        df1 = pd.read_csv(filename,header=[0])\n",
    "    else:\n",
    "        df1=pd.read_json(filename,orient=\"records\", lines=True)\n",
    "    return list(df1.columns) \n",
    "\n",
    "def split_large_file(filename, source, output,size=None):\n",
    "    df1=pd.read_csv(filename,header=[0])\n",
    "    if size ==None:\n",
    "        size = get_csv_size(filename)\n",
    "    num_chunks = size//20\n",
    "    if num_chunks == 0:\n",
    "        num_chunks = 1 \n",
    "    df_all = np.array_split(df1, num_chunks)\n",
    "\n",
    "    for idx, file in enumerate(df_all):\n",
    "        file.to_csv('%s/%s_%03d.csv'%(output, source, idx), index=False)   \n",
    "        \n",
    "def show_all_files(folder):\n",
    "    df = pd.DataFrame(glob.glob('%s/*'%folder), columns = ['path'])\n",
    "    df['root'] = df.path.apply(lambda x: x.split('/')[0])\n",
    "    df['source'] = df.path.apply(lambda x: x.replace('%s/'%folder,'').split('_')[0])\n",
    "    df['filename'] = df.path.apply(lambda x: x.replace('%s/'%folder,'').split('/')[-1])\n",
    "    df = df.sort_values('source').reset_index(drop=True)\n",
    "    df['size'] = df['path'].apply(get_csv_size)\n",
    "    return df\n",
    "\n",
    "def unicodetoascii(text):\n",
    "    TEXT = (text.\n",
    "    \t\treplace('\\\\xe2\\\\x80\\\\x99', \"'\").\n",
    "            replace('\\\\xc3\\\\xa9', 'e').\n",
    "            replace('\\\\xe2\\\\x80\\\\x90', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x91', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x92', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x93', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x94', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x94', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x98', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\x9b', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\x9c', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9c', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9d', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9e', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9f', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\xa6', '...').\n",
    "            replace('\\\\xe2\\\\x80\\\\xb2', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb3', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb4', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb5', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb6', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb7', \"'\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xba', \"+\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbb', \"-\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbc', \"=\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbd', \"(\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbe', \")\"))\n",
    "    return TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d01ca",
   "metadata": {
    "id": "990d01ca"
   },
   "source": [
    "# Check csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3c0e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T16:59:28.416272Z",
     "start_time": "2021-04-16T16:59:28.360377Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 156,
     "status": "error",
     "timestamp": 1658175105651,
     "user": {
      "displayName": "Vikas Thoutam",
      "userId": "14762817719019055278"
     },
     "user_tz": 300
    },
    "id": "86d3c0e2",
    "outputId": "24c48d86-e896-4da2-d3c9-b38387ef3eac",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pandarallel.initialize(nb_workers=94, progress_bar=True) \n",
    "# Place all files to be filtered within the Filter folder, within a subfolder containing the source name\n",
    "# for example, let there be 3 csv files from El Espectador. \n",
    "# The file structure is \n",
    "# Filter\n",
    "# --elespectador\n",
    "# -- -- elespactador01.csv\n",
    "# -- -- elespactador02.csv\n",
    "# -- -- elespactador03.csv\n",
    "\n",
    "\n",
    "# This creates a dataframe containing a list of all the csv files within the specified folder structure.\n",
    "df = pd.DataFrame(glob.glob('*/*.csv'), columns = ['path'])\n",
    "#print(df)\n",
    "# This updates the source to be just the first part, the name of the folder.\n",
    "df['source'] = df.path.apply(lambda x: x.split('/')[1].replace('.csv', ''))\n",
    "df = df.sort_values('source').reset_index(drop=True)\n",
    "# Gets the column defenitions of the files and their sizes\n",
    "df['attributes']= df.path.apply(get_attribute)\n",
    "df['size']= df.path.apply(get_csv_size)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610e6985",
   "metadata": {},
   "source": [
    "Remove unrequired columns like 'Unnamed: 0' and 'Unnamed: 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e6c23",
   "metadata": {},
   "outputs": [],
   "source": [
    " for p in df.path:\n",
    "     current_df = pd.read_csv(p, index_col=0)\n",
    "     if 'Unnamed: 0' in current_df.columns:\n",
    "         current_df = current_df.drop(['Unnamed: 0'], axis=1)\n",
    "        \n",
    "     if 'Unnamed: 1' in current_df.columns:\n",
    "         current_df = current_df.drop(['Unnamed: 1'], axis=1)\n",
    "        \n",
    "     if 'Unnamed: 0.1' in current_df.columns:\n",
    "         current_df = current_df.drop(['Unnamed: 0.1'], axis=1)\n",
    "     current_df.to_csv(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe2fd5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df = df[df.attributes.map(len)==6] #Only get the CSVs which have the appropriate number of columns\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec40c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['size'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb652d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_another = pd.DataFrame(glob.glob('split/*_filtered.csv'), columns = ['path'])\n",
    "# df_another['size']= df_another.path.apply(get_csv_size)\n",
    "# df_another['size'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LmLUazUi8hky",
   "metadata": {
    "id": "LmLUazUi8hky"
   },
   "source": [
    "Initialize filter files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81989dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Trie():\n",
    "    \"\"\"Regex::Trie in Python. Creates a Trie out of a list of words. The trie can be exported to a Regex pattern.\n",
    "    The corresponding Regex should match much faster than a simple Regex union.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def add(self, word):\n",
    "        ref = self.data\n",
    "        for char in word:\n",
    "            ref[char] = char in ref and ref[char] or {}\n",
    "            ref = ref[char]\n",
    "        ref[''] = 1\n",
    "\n",
    "    def dump(self):\n",
    "        return self.data\n",
    "\n",
    "    def quote(self, char):\n",
    "        return re.escape(char)\n",
    "\n",
    "    def _pattern(self, pData):\n",
    "        data = pData\n",
    "        if \"\" in data and len(data.keys()) == 1:\n",
    "            return None\n",
    "\n",
    "        alt = []\n",
    "        cc = []\n",
    "        q = 0\n",
    "        for char in sorted(data.keys()):\n",
    "            if isinstance(data[char], dict):\n",
    "                try:\n",
    "                    recurse = self._pattern(data[char])\n",
    "                    alt.append(self.quote(char) + recurse)\n",
    "                except:\n",
    "                    cc.append(self.quote(char))\n",
    "            else:\n",
    "                q = 1\n",
    "        cconly = not len(alt) > 0\n",
    "\n",
    "        if len(cc) > 0:\n",
    "            if len(cc) == 1:\n",
    "                alt.append(cc[0])\n",
    "            else:\n",
    "                alt.append('[' + ''.join(cc) + ']')\n",
    "\n",
    "        if len(alt) == 1:\n",
    "            result = alt[0]\n",
    "        else:\n",
    "            result = \"(?:\" + \"|\".join(alt) + \")\"\n",
    "\n",
    "        if q:\n",
    "            if cconly:\n",
    "                result += \"?\"\n",
    "            else:\n",
    "                result = \"(?:%s)?\" % result\n",
    "        return result\n",
    "\n",
    "    def pattern(self):\n",
    "        return self._pattern(self.dump())\n",
    "\n",
    "def trie_regex_from_words(words):\n",
    "    trie = Trie()\n",
    "    for word in words:\n",
    "        trie.add(word)\n",
    "    return re.compile(r\"\\b\" + trie.pattern() + r\"\\b\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730ee7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the relevant keywords and the irrelevant keyword regex's\n",
    "with open('wiki_relevant_arabic_v2.txt', 'r', encoding='utf8') as file:    \n",
    "    relevant_rules = file.read().split('\\n')\n",
    "relevant_rules = '\\\\b' + '|\\\\b'.join(relevant_rules)\n",
    "\n",
    "# relevant_rules = re.compile(relevant_rules, re.IGNORECASE)\n",
    "print(relevant_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0cb4d0-bf7e-4ac9-a68a-2746fef10a17",
   "metadata": {
    "id": "2d0cb4d0-bf7e-4ac9-a68a-2746fef10a17",
    "outputId": "0142fea4-d15c-420a-eae0-5bcbaf931df9"
   },
   "outputs": [],
   "source": [
    "# Initialize the relevant keywords and the irrelevant keyword regex's\n",
    "with open('wiki_relevant_arabic_v2.txt', 'r', encoding='utf8') as file:    \n",
    "    relevant_rules = file.read().split('\\n')\n",
    "relevant_rules = '\\\\b' + '|\\\\b'.join(relevant_rules)\n",
    "\n",
    "if relevant_rules.endswith('|\\\\b'):\n",
    "    relevant_rules =relevant_rules[:-3]   \n",
    "\n",
    "relevant_rules = re.compile(relevant_rules, re.IGNORECASE)\n",
    "\n",
    "print('relevant_rules:')\n",
    "print(relevant_rules)\n",
    "\n",
    "with open('irrelevant_keywords_arabic_v2.txt', 'r', encoding='utf8') as file:    \n",
    "    irelevant_rules = file.read().split('\\n')\n",
    "irelevant_rules = '\\\\b' + '|\\\\b'.join(irelevant_rules)\n",
    "\n",
    "if irelevant_rules.endswith('|\\\\b'):\n",
    "    irelevant_rules =irelevant_rules[:-3]   \n",
    "\n",
    "irelevant_rules = re.compile(irelevant_rules, re.IGNORECASE)\n",
    "\n",
    "print('\\nirelevant_rules:')\n",
    "print(irelevant_rules)\n",
    "\n",
    "# debugging, no longer relevant\n",
    "#find_relevant = lambda x: Counter(relevant_rules.findall(str(x).lower()))\n",
    "#find_exclude = lambda x: Counter(irelevant_rules.findall(str(x).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99122a34-8538-43e3-a2be-a64f59118866",
   "metadata": {
    "id": "99122a34-8538-43e3-a2be-a64f59118866"
   },
   "outputs": [],
   "source": [
    "# remove all the filtered paths, if they exist. Debugging\n",
    "# df['path'] = df['path'].apply(lambda x: x[:-4] + '_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86a45a-42a7-4a4c-b259-4f565d8ade91",
   "metadata": {
    "id": "bb86a45a-42a7-4a4c-b259-4f565d8ade91"
   },
   "outputs": [],
   "source": [
    "# Checks for doublely filtered files. Debugging\n",
    "df = df[~df.path.str.contains('_filtered_filtered')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd5676",
   "metadata": {
    "id": "5fcd5676"
   },
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68637012",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad74114",
   "metadata": {
    "id": "3ad74114",
    "outputId": "46b4d4c1-c042-4761-900d-cc3e954dab30"
   },
   "outputs": [],
   "source": [
    "# Makes directory for splits. Do not run if a split folder already exists.\n",
    "cwd = os.getcwd()\n",
    "#cwd = '/xdisk/josorio1/salsarra/Filtered/'\n",
    "os.mkdir(cwd + '/split/')\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34473839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T17:01:17.485360Z",
     "start_time": "2021-04-16T16:59:30.463879Z"
    },
    "id": "34473839",
    "outputId": "369d93b0-2d84-4ce4-9f63-37cf9032db87",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# splits all files above a certain MB limit\n",
    "pandarallel.initialize(nb_workers=94, progress_bar=True)       \n",
    "_ = df.apply(lambda x: split_large_file(x['path'],x['source'],'split'), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c557d03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T17:01:18.244896Z",
     "start_time": "2021-04-16T17:01:17.488808Z"
    },
    "id": "2c557d03",
    "outputId": "d5b893dc-dfac-4b5d-fc6e-ec3f90ec901d"
   },
   "outputs": [],
   "source": [
    "# Debugging, if there are issues here, let me know.\n",
    "df = show_all_files('split')\n",
    "df = df[~df.path.str.contains('_filtered')]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97afac67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T17:01:18.260861Z",
     "start_time": "2021-04-16T17:01:18.246535Z"
    },
    "id": "97afac67",
    "outputId": "dbe431bf-e848-4340-a896-31bc0e58d564"
   },
   "outputs": [],
   "source": [
    "# Check file sizes.\n",
    "df['size'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58bb83b",
   "metadata": {
    "id": "f58bb83b"
   },
   "source": [
    "# step 1  clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36381c42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T17:01:19.018083Z",
     "start_time": "2021-04-16T17:01:18.262396Z"
    },
    "id": "36381c42",
    "outputId": "241264ae-5408-4ed3-e752-11db8a9646d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = show_all_files('split')\n",
    "# df['attribute'] = df.path.parallel_apply(get_attribute)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3ce25",
   "metadata": {
    "id": "9fd3ce25",
    "outputId": "3a0efd92-deda-433a-ba7f-017c50fa38f2"
   },
   "outputs": [],
   "source": [
    "df.source.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9db0b2",
   "metadata": {
    "id": "db9db0b2",
    "outputId": "825cdd49-6cc6-48bf-b9b0-ecdae35e36be"
   },
   "outputs": [],
   "source": [
    "print('total size:', df['size'].sum())\n",
    "df.groupby(by=[\"source\"], dropna=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef00fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T17:01:19.031335Z",
     "start_time": "2021-04-16T17:01:19.019496Z"
    },
    "code_folding": [],
    "id": "03ef00fd"
   },
   "outputs": [],
   "source": [
    "# Cleans the given files.\n",
    "def basic_process(filename, output_folder):\n",
    "    \n",
    "    sizes = []\n",
    "    \n",
    "    df1 = pd.read_csv(filename, header=[0], index_col=0)\n",
    "    sizes.append(df1.shape[0])\n",
    "    \n",
    "    if 'url' not in df1.columns:\n",
    "        df1['url'] ='\\n'\n",
    "    if 'title' not in df1.columns:\n",
    "        df1['title']='\\n'\n",
    "    \n",
    "#     df1 = df1[~df1.url.isnull()]\n",
    "    \n",
    "    df1 = df1[~df1.text.isnull()]\n",
    "    df1 = df1[~df1.text.duplicated()]\n",
    "#     df1 = df1[~df1.url.duplicated()]\n",
    "    \n",
    "    df1.loc[df1.title.isnull(),'title']='\\n'\n",
    "    df1.loc[df1.url.isnull(),'url']='\\n'\n",
    "    if 'abstract' in df1:\n",
    "        df1.loc[df1.abstract.isnull(),'abstract']='\\n'\n",
    "        \n",
    "    # ------------------- Start cleaning  --------------------------#\n",
    "    \n",
    "    # Convert coding\n",
    "    df1.text = df1.text.apply(lambda x: unicodetoascii(x))\n",
    "    df1.text = df1.text.apply(lambda x: unicodedata.normalize(\"NFKD\", x))\n",
    "    \n",
    "    # email\n",
    "    df1.text = df1.text.apply(lambda x: re.sub(\"\\S+@\\S+(?:\\.\\S+)+\",'',x))\n",
    "    \n",
    "    # telphone\n",
    "    df1.text = df1.text.apply(lambda x: re.sub('\\(\\+( |-|\\d)+\\)( |-|\\d)+',' ',x))\n",
    "    df1.text = df1.text.apply(lambda x: re.sub('\\+( |-|\\d)+',' ',x))\n",
    "    \n",
    "    # noise\n",
    "    df1.text =\\\n",
    "    df1.text.apply(lambda x: re.sub('\\n(ad|advertisement|tweet):?\\n', \"\", x, flags=re.IGNORECASE))\n",
    "    \n",
    "    # urls\n",
    "    df1.text = df1.text.apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
    "    \n",
    "    # delete too many \\n\n",
    "    df1.text = df1.text.apply(lambda x: re.sub('\\n\\n+', \"\\n\\n\", x, flags=re.IGNORECASE))\n",
    "    \n",
    "    # head and tails\n",
    "    df1.text = df1.text.apply(lambda x: re.sub(\"^\\s+|\\s+$\", \"\", x, flags=re.UNICODE)) \n",
    "    \n",
    "    \n",
    "    df1 = df1[df1.text.str.len()>100]\n",
    "    \n",
    "    sizes.append(df1.shape[0])\n",
    "    \n",
    "    # ------------------- Ending cleaning  --------------------------#\n",
    "    \n",
    "    filename = filename.split('/')[1]\n",
    "    new_filename = output_folder + '/'+ filename\n",
    "\n",
    "    print('%s:\\t%s'%(filename, sizes))\n",
    "    df1.to_csv(new_filename, index= False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7707fcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T17:03:25.696688Z",
     "start_time": "2021-04-16T17:01:19.032425Z"
    },
    "id": "e7707fcd",
    "outputId": "57ebce38-cd53-4f4a-93ff-3b5568ab950d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Takes the splits and cleans them.\n",
    "pandarallel.initialize(nb_workers=94, progress_bar=False) \n",
    "_ = df['path'].apply(basic_process, output_folder='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862bec9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T17:03:25.885526Z",
     "start_time": "2021-04-16T17:03:25.700419Z"
    },
    "id": "862bec9c",
    "outputId": "4095e280-8ffa-4328-d24f-dcb4659ff2d9"
   },
   "outputs": [],
   "source": [
    "df = show_all_files('split')\n",
    "df = df[~df.path.str.contains('filtered')]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60857c57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T17:03:25.905012Z",
     "start_time": "2021-04-16T17:03:25.888171Z"
    },
    "id": "60857c57",
    "outputId": "2b048c50-7ca9-4941-f0fd-615907955e06"
   },
   "outputs": [],
   "source": [
    "print('total size:', df['size'].sum())\n",
    "df.groupby(by=[\"source\"], dropna=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BGGg5UFA9jyD",
   "metadata": {
    "id": "BGGg5UFA9jyD"
   },
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85959b26-4cdc-4550-a417-1a02a9122ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a metric for number of relevant and irrelevant words.\n",
    "def find_relevant(str):\n",
    "    return len(relevant_rules.findall(str))/len(str.split())\n",
    "def find_irelevant(str):\n",
    "    return len(irelevant_rules.findall(str))/len(str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c32c2-8b9c-49b7-a799-b050f5e481d0",
   "metadata": {
    "id": "9b7c32c2-8b9c-49b7-a799-b050f5e481d0",
    "outputId": "4104500f-7c73-4335-9b9c-57cc80cf878f"
   },
   "outputs": [],
   "source": [
    "# find_relevant = lambda x: Counter(relevant_rules.findall(x.lower()))\n",
    "# find_exclude = lambda x: Counter(irelevant_rules.findall(x.lower()))\n",
    "#\n",
    "\n",
    "# for each file, open and read into dataframe, then make sure text is NOT null\n",
    "def filter(filepath, filter_for_positive=True):\n",
    "    print(f'Filter for positive is {filter_for_positive}')\n",
    "    sizes = []\n",
    "    df1 = pd.read_csv(filepath, header=[0], index_col=0)\n",
    "    if 'text' not in df1.columns:\n",
    "        print(f'{filepath} has no text column')\n",
    "        return \n",
    "    sizes.append(df1.shape[0]) \n",
    "    print(filepath)\n",
    "    print(len(df1))\n",
    "    print(df1.columns)\n",
    "    df1 = df1[~df1.text.isnull()]\n",
    "    print(len(df1))\n",
    "#     df1 = df1[~df1['text'] == '']\n",
    "    print(len(df1))\n",
    "    df1['relevant'] = df1['text'].apply(find_relevant)\n",
    "    df1['irrelevant'] = df1['text'].apply(find_irelevant)\n",
    "\n",
    "    # df1['ratio'] = df1['relevant']/df1['irrelevant']\n",
    "    relevant_count = df1.relevant\n",
    "    irrelevant_count = df1.irrelevant\n",
    "    \n",
    "\n",
    "    # ratio_values = df1.ratio\n",
    "# Filter Calculation: Mean and SD of both relevant and irrelevant columns\n",
    "    relevant_mean = np.mean(df1['relevant'])\n",
    "    irrelevant_mean = np.mean(df1['irrelevant'])\n",
    "    relevant_sd = np.std(df1['relevant'])\n",
    "    irrelevant_sd = np.std(df1['irrelevant'])\n",
    "    # print(np.mean(df1['ratio']), np.std(df1['ratio']))\n",
    "    # print(df1['relevant'].describe())\n",
    "    print(relevant_mean, relevant_sd)\n",
    "    # print(df1['irrelevant'].describe())\n",
    "    print(irrelevant_mean, irrelevant_sd) \n",
    "# The next three lines are the filter, can be changed based on the needs of the project. Currently, throw out all rows with more than two irrelevant words/regex\n",
    "    threshold = 1.5\n",
    "    # filter based on ratio rather than just counts    \n",
    "    select_index = (relevant_count >= irrelevant_count) & (irrelevant_count < irrelevant_mean + threshold * irrelevant_sd) & (relevant_count > relevant_mean - threshold * relevant_sd) # (ratio_values > np.mean(df1['ratio']) - threshold * np.std(df1['ratio']))\n",
    "    select_index = [x if filter_for_positive else not x for x in select_index]\n",
    "\n",
    "    df1 = df1[select_index]   \n",
    "    df1 = df1.drop('relevant', axis=1)\n",
    "    df1 = df1.drop('irrelevant', axis=1)\n",
    "    # df1 = df1.drop('ratio', axis=1)\n",
    "    sizes.append(df1.shape[0])\n",
    "# next line is debugging    \n",
    "    # print(df1)\n",
    "# create a filtered file with the given dataframe, will appear in same folder as unfiltered\n",
    "    \n",
    "    if filter_for_positive:  \n",
    "        df1.to_csv(filepath[:-4] + '_filtered.csv', index=False)\n",
    "    else:\n",
    "        df1.to_csv(filepath[:-4] + '_irrelevant_filtered.csv', index=False)\n",
    "    print('%s:\\t%s'%(filepath[6:-4], sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71299c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter(\"split/Guatemala_000.csv\")\n",
    "# df_x = pd.read_csv(\"split/Guatemala_000.csv\", header=[0], index_col=0)\n",
    "# # df_x.iloc[:, 0]\n",
    "# # df_x.columns\n",
    "# df_x[[:, 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb6f7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandarallel.initialize(nb_workers=94, progress_bar=True) \n",
    "_ = df['path'].parallel_apply(filter, filter_for_positive=True) #Get relevant articles\n",
    "\n",
    "_ = df['path'].parallel_apply(filter, filter_for_positive=False) #Get irrelevant articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae742ab3",
   "metadata": {
    "id": "ae742ab3"
   },
   "source": [
    "# save to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e58dd",
   "metadata": {
    "id": "867e58dd",
    "outputId": "54ec73ba-e33e-48b2-8313-deaf3e3a3436"
   },
   "outputs": [],
   "source": [
    "# folder = 'none'\n",
    "# df = pd.DataFrame(columns=['path','source', 'filename'])\n",
    "# df.path = [str(x) for x in pathlib.Path('%s/*'%folder).glob('**/*')]\n",
    "df = pd.DataFrame(glob.glob('*/*'), columns = ['path'])\n",
    "df = df[df.path.str.contains('_filtered')]\n",
    "# df['root'] = df.path.apply(lambda x: x.split('/')[0])\n",
    "\n",
    "df['source'] = df.path.apply(lambda x: x.split('/')[1][0:x.index('_')-6])\n",
    "df['filename'] = df.path.apply(lambda x: x.split('/')[-1])\n",
    "\n",
    "# df = df.sort_values('source').reset_index(drop=True)\n",
    "# df['size'] = df['path'].parallel_apply(get_csv_size)\n",
    "# df['filename'] = df.path.apply(lambda x: x.replace('%s/'%folder,''))\n",
    "\n",
    "df['json_file'] = df['filename'].apply(lambda x: x.replace('.csv', '.json'))\n",
    "\n",
    "df['json_file'] = df.source +'/'+ df.json_file\n",
    "df['tar_file'] = 'tar/'+ df.json_file+'.tar.gz'\n",
    "df.json_file = 'json/'+ df.json_file\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f5ca9",
   "metadata": {
    "id": "141f5ca9",
    "outputId": "0bc8b412-6673-493c-93c5-9f2e5099937a"
   },
   "outputs": [],
   "source": [
    "os.mkdir(cwd + '/json/')\n",
    "os.mkdir(cwd + '/tar/')\n",
    "\n",
    "for i in df.source.unique():\n",
    "    os.mkdir(cwd + '/json/'+i)\n",
    "    os.mkdir(cwd + '/tar/'+i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481075a",
   "metadata": {
    "id": "7481075a",
    "outputId": "b4b5a075-56b1-4dbb-a1b4-39a22b1c19d1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(df.path[1], header=[0], index_col=0)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf856f89",
   "metadata": {
    "id": "cf856f89",
    "outputId": "313c4a3e-2803-4db1-e859-5e9f68b59f94"
   },
   "outputs": [],
   "source": [
    "#----------------------this was commented out-----------\n",
    "# def convert_json_tar(filename, json_file, tar_file):\n",
    "#     df1 = pd.read_csv(filename, header=[0], index_col=0)      \n",
    "#     df1.to_json(json_file, orient=\"records\", lines=True)\n",
    "    \n",
    "#     with tarfile.open(tar_file, \"w:gz\") as tar:\n",
    "#         tar.add(json_file, arcname=os.path.basename(json_file))\n",
    "# df.apply(lambda x: convert_json_tar(x['path'], x['json_file'], x['tar_file']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc159d9",
   "metadata": {
    "id": "bdc159d9",
    "outputId": "4903be10-5801-4fdc-ba9e-59ba69679ae8"
   },
   "outputs": [],
   "source": [
    "pandarallel.initialize() \n",
    "\n",
    "df.source = df.path.apply(lambda x: x.split('/')[1][0:x.index('_')-6]).values\n",
    "df['size'] = df['path'].apply(get_csv_size)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6ed32d-9b1f-4080-bbd9-e2713589229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in next(os.walk(\"split\"), (_, _, []))[2]:\n",
    "    if filename.endswith(\"irrelevant_filtered.csv\"):\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b98bb6",
   "metadata": {
    "id": "92b98bb6",
    "outputId": "9bbdebe2-a3f4-47c0-fef5-87f58eac6f23"
   },
   "outputs": [],
   "source": [
    "print(df['size'].sum())\n",
    "\n",
    "df.groupby(by=[\"path\"], dropna=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acaa5a3-d43c-446d-bf5c-46a9271d8d26",
   "metadata": {
    "id": "0acaa5a3-d43c-446d-bf5c-46a9271d8d26"
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for filename in next(os.walk(\"split\"), (_, _, []))[2]:\n",
    "    filename = f\"split/{filename}\"\n",
    "    if filename.endswith(\"_filtered.csv\"):\n",
    "        l.append(pd.read_csv(filename, index_col=0))\n",
    "df = pd.concat(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4162635",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Unnamed: 0' in df:\n",
    "    df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "df.to_csv(\"split/all_irrelevant_filtered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782b937",
   "metadata": {},
   "source": [
    "Get all the irrelevant CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf52f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for filename in next(os.walk(\"split\"), (_, _, []))[2]:\n",
    "    filename = f\"split/{filename}\"\n",
    "    if filename.endswith(\"irrelevant_filtered.csv\"):\n",
    "        l.append(pd.read_csv(filename, index_col=0))\n",
    "df = pd.concat(l)\n",
    "\n",
    "if 'Unnamed: 0' in df:\n",
    "    df = df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "df.to_csv(\"split/all_irrelevant_filtered.csv\")\n",
    "\n",
    "seconds = time.time() - t0\n",
    "duration = time.strftime(\"(%H:%M:%S)\",time.gmtime(seconds))\n",
    "print('Time to complete:',duration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83674be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8846300b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PanamaProcess.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.776px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
